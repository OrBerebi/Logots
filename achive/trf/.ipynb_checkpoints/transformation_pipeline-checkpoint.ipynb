{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a29cd69",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ast\n",
    "import time\n",
    "from pathlib import Path\n",
    "from scipy.fftpack import fft, fftfreq\n",
    "from scipy.signal import spectrogram\n",
    "import cv2, base64, io, warnings\n",
    "from PIL import Image\n",
    "from ultralytics import YOLO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05cfad21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 1 – Build `trans_audio_features` from stg_audio_data.csv\n",
    "# ------------------------------------------------------------------\n",
    "def compute_fft(samples, sampling_rate=8000):\n",
    "    \"\"\"Return frequencies and magnitudes for the given audio sample list.\"\"\"\n",
    "    samples = np.asarray(samples, dtype=float)\n",
    "    n = len(samples)\n",
    "    freqs = np.fft.rfftfreq(n, d=1 / sampling_rate)\n",
    "    fft_values = np.abs(np.fft.rfft(samples))\n",
    "    return { \"freqs\": freqs, \"magnitudes\": fft_values }\n",
    "\n",
    "def classify_voice_or_noise(freqs, magnitudes,\n",
    "                            voice_freq_range=(500, 3500),\n",
    "                            energy_threshold=1.5e7):\n",
    "    \"\"\"Very simple energy‑based binary classification.\"\"\"\n",
    "    # total energy in voice band\n",
    "    band = (freqs >= voice_freq_range[0]) & (freqs <= voice_freq_range[1])\n",
    "    energy = magnitudes[band].sum()\n",
    "    return \"Voice\" if energy > energy_threshold else \"Noise\"\n",
    "\n",
    "def detect_cat_voice(classification, freqs, magnitudes,\n",
    "                     freq_range=(400, 700), harmonic_range=(200, 1000),\n",
    "                     harmonic_threshold=15):\n",
    "    \"\"\"Heuristic cat meow detector based on dominant frequency and harmonic energy.\"\"\"\n",
    "    if classification != \"Voice\":\n",
    "        return False\n",
    "    # strongest peak\n",
    "    dom_freq = freqs[np.argmax(magnitudes)]\n",
    "    if not (freq_range[0] <= dom_freq <= freq_range[1]):\n",
    "        return False\n",
    "    # harmonic energy\n",
    "    harm_band = (freqs >= harmonic_range[0]) & (freqs <= harmonic_range[1])\n",
    "    if magnitudes[harm_band].mean() < harmonic_threshold:\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "def detect_human_voice(classification, freqs, magnitudes,\n",
    "                       freq_range=(150, 600), harmonic_range=(2000, 4000),\n",
    "                       harmonic_threshold=15):\n",
    "    \"\"\"Very coarse human‑speech detector (adult male/female fundamental).\"\"\"\n",
    "    if classification != \"Voice\":\n",
    "        return False\n",
    "    dom_freq = freqs[np.argmax(magnitudes)]\n",
    "    return freq_range[0] <= dom_freq <= freq_range[1]\n",
    "\n",
    "def calculate_meow_loudness(is_cat_voice, magnitudes):\n",
    "    return magnitudes.max() if is_cat_voice else 0.0\n",
    "\n",
    "def calculate_dominant_frequency(freqs, magnitudes):\n",
    "    return freqs[np.argmax(magnitudes)]\n",
    "\n",
    "def build_trans_audio_features(csv_path: str = \"stg_audio_data.csv\"):\n",
    "    stg_audio_data = pd.read_csv(csv_path)\n",
    "    # audio_samples stored as stringified Python lists -> convert\n",
    "    stg_audio_data[\"audio_samples\"] = stg_audio_data[\"audio_samples\"].apply(ast.literal_eval)\n",
    "\n",
    "    results = []\n",
    "    for _, row in stg_audio_data.iterrows():\n",
    "        fft_res       = compute_fft(row[\"audio_samples\"])\n",
    "        classification = classify_voice_or_noise(fft_res[\"freqs\"], fft_res[\"magnitudes\"])\n",
    "        is_cat        = detect_cat_voice(classification, fft_res[\"freqs\"], fft_res[\"magnitudes\"])\n",
    "        is_human      = detect_human_voice(classification, fft_res[\"freqs\"], fft_res[\"magnitudes\"])\n",
    "        meow_loudness = calculate_meow_loudness(is_cat, fft_res[\"magnitudes\"])\n",
    "        dom_freq      = calculate_dominant_frequency(fft_res[\"freqs\"], fft_res[\"magnitudes\"])\n",
    "        results.append({\n",
    "            \"frame_id\"        : row[\"frame_id\"],\n",
    "            \"timestamp\"       : row[\"timestamp\"],\n",
    "            \"classification\"  : classification,\n",
    "            \"is_cat_voice\"    : is_cat,\n",
    "            \"is_human_voice\"  : is_human,\n",
    "            \"meow_loudness\"   : meow_loudness,\n",
    "            \"dominant_frequency\": dom_freq,\n",
    "        })\n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "trans_audio_features = build_trans_audio_features()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89101680",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 2 – Build `trans_imu_features` from stg_imu_data.csv\n",
    "# ------------------------------------------------------------------\n",
    "def unwrap_yaw(yaw_list):\n",
    "    arr = np.asarray(yaw_list, dtype=float)\n",
    "    return np.degrees(np.unwrap(np.radians(arr)))\n",
    "\n",
    "def avg_intra_yaw_diff(yaw_list):\n",
    "    unwrapped = unwrap_yaw(yaw_list)\n",
    "    return np.mean(np.diff(unwrapped)) if len(unwrapped) > 1 else 0.0\n",
    "\n",
    "def compute_rotation_speed(current_yaw_list, prev_avg_yaw):\n",
    "    \"\"\"Return rotation speed (deg/frame), current average yaw and delta yaw.\"\"\"\n",
    "    current_avg_yaw = avg_intra_yaw_diff(current_yaw_list)\n",
    "    if prev_avg_yaw is None:\n",
    "        return 0.0, current_avg_yaw, 0.0\n",
    "    delta_yaw = current_avg_yaw - prev_avg_yaw\n",
    "    rotation_speed = abs(delta_yaw)\n",
    "    return rotation_speed, current_avg_yaw, delta_yaw\n",
    "\n",
    "def compute_movement_intensity(delta_yaw, delta_pitch, delta_roll):\n",
    "    return np.sqrt(delta_yaw**2 + delta_pitch**2 + delta_roll**2)\n",
    "\n",
    "def compute_balance_state(pitch, roll, movement_intensity,\n",
    "                          pitch_thr=15, roll_thr=15, move_thr=10):\n",
    "    \"\"\"Simple heuristic for balance state.\"\"\"\n",
    "    if movement_intensity > move_thr:\n",
    "        return \"moving\"\n",
    "    if abs(pitch) < pitch_thr and abs(roll) < roll_thr:\n",
    "        return \"balanced\"\n",
    "    return \"unbalanced\"\n",
    "\n",
    "def compute_cat_interaction(movement_intensity, move_thr=20):\n",
    "    return movement_intensity > move_thr\n",
    "\n",
    "def process_imu_live(imu_df):\n",
    "    state = { \"prev_avg_yaw\": None, \"prev_pitch\": None, \"prev_roll\": None }\n",
    "    rows = []\n",
    "    for _, row in imu_df.iterrows():\n",
    "        rotation_speed, current_avg_yaw, delta_yaw = compute_rotation_speed(row[\"yaw\"], state[\"prev_avg_yaw\"])\n",
    "        pitch = row[\"pitch\"]\n",
    "        roll  = row[\"roll\"]\n",
    "        delta_pitch = 0.0 if state[\"prev_pitch\"] is None else pitch - state[\"prev_pitch\"]\n",
    "        delta_roll  = 0.0 if state[\"prev_roll\"]  is None else roll  - state[\"prev_roll\"]\n",
    "\n",
    "        movement_intensity = compute_movement_intensity(delta_yaw, delta_pitch, delta_roll)\n",
    "        balance_state      = compute_balance_state(pitch, roll, movement_intensity)\n",
    "        cat_interaction    = compute_cat_interaction(movement_intensity)\n",
    "\n",
    "        rows.append({\n",
    "            \"frame_id\"           : row[\"frame_id\"],\n",
    "            \"timestamp\"          : row[\"timestamp\"],\n",
    "            \"rotation_speed\"     : rotation_speed,\n",
    "            \"delta_yaw\"          : delta_yaw,\n",
    "            \"delta_pitch\"        : delta_pitch,\n",
    "            \"delta_roll\"         : delta_roll,\n",
    "            \"movement_intensity\" : movement_intensity,\n",
    "            \"balance_state\"      : balance_state,\n",
    "            \"cat_interaction_detected\": cat_interaction\n",
    "        })\n",
    "\n",
    "        state[\"prev_avg_yaw\"] = current_avg_yaw\n",
    "        state[\"prev_pitch\"]   = pitch\n",
    "        state[\"prev_roll\"]    = roll\n",
    "\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "def build_trans_imu_features(csv_path: str = \"stg_imu_data.csv\"):\n",
    "    stg_imu_data = pd.read_csv(csv_path)\n",
    "    # Convert string lists to Python lists\n",
    "    for col in [\"yaw\", \"pitch\", \"roll\"]:\n",
    "        stg_imu_data[col] = stg_imu_data[col].apply(ast.literal_eval)\n",
    "    # pitch & roll scalar (take first sample)\n",
    "    stg_imu_data[\"pitch\"] = stg_imu_data[\"pitch\"].apply(lambda x: x[0] if isinstance(x, list) else x)\n",
    "    stg_imu_data[\"roll\"]  = stg_imu_data[\"roll\"].apply(lambda x: x[0] if isinstance(x, list) else x)\n",
    "    # yaw remain list\n",
    "    return process_imu_live(stg_imu_data)\n",
    "\n",
    "trans_imu_features = build_trans_imu_features()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dfd7143",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 3 – Build `trans_visual_cat_detection` from stg_visual_data.csv\n",
    "# ------------------------------------------------------------------\n",
    "IMG_SIZE = 640\n",
    "CONF_THR = 0.05\n",
    "DEVICE   = \"cpu\"\n",
    "CAT_ID   = 15\n",
    "\n",
    "# initialise YOLO model (cat‑only)\n",
    "_yolo_model = YOLO(\"yolov8n.pt\").to(DEVICE)\n",
    "_yolo_model.fuse()\n",
    "_yolo_model.overrides[\"conf\"]    = CONF_THR\n",
    "_yolo_model.overrides[\"classes\"] = [CAT_ID]\n",
    "\n",
    "import base64, io\n",
    "\n",
    "def jpeg_b64_to_rgb_ndarray(b64: str, img_size: int = IMG_SIZE):\n",
    "    with Image.open(io.BytesIO(base64.b64decode(b64))) as im:\n",
    "        im = im.convert(\"RGB\").resize((img_size, img_size), Image.LANCZOS)\n",
    "        return np.asarray(im, dtype=np.uint8)\n",
    "\n",
    "def detect_cat(df: pd.DataFrame, img_size: int = IMG_SIZE) -> pd.DataFrame:\n",
    "    rows = []\n",
    "    for _, r in df.iterrows():\n",
    "        rgb = jpeg_b64_to_rgb_ndarray(r[\"frame_data\"], img_size)\n",
    "        res = _yolo_model(rgb, imgsz=img_size, verbose=False)[0]\n",
    "        boxes = res.boxes.cpu()\n",
    "        det_pd = pd.DataFrame({\n",
    "            \"xmin\"      : boxes.xyxy[:, 0].numpy(),\n",
    "            \"ymin\"      : boxes.xyxy[:, 1].numpy(),\n",
    "            \"xmax\"      : boxes.xyxy[:, 2].numpy(),\n",
    "            \"ymax\"      : boxes.xyxy[:, 3].numpy(),\n",
    "            \"confidence\": boxes.conf.numpy(),\n",
    "            \"class\"     : boxes.cls.numpy().astype(int),\n",
    "            \"name\"      : [\"cat\"]*len(boxes),\n",
    "        })\n",
    "        cats = det_pd\n",
    "        rows.append({\n",
    "            \"frame_id\"        : int(r[\"frame_id\"]),\n",
    "            \"timestamp\"       : r[\"timestamp\"],\n",
    "            \"is_cat_detected\" : int(len(cats) > 0),\n",
    "            \"cat_confidence\"  : float(cats[\"confidence\"].max()) if len(cats) else 0.0,\n",
    "            \"raw_detection\"   : det_pd.to_dict(\"records\"),\n",
    "        })\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "def build_trans_visual_cat_detection(csv_path: str = \"stg_visual_data.csv\"):\n",
    "    stg_visual_data = pd.read_csv(csv_path, converters={\"frame_data\": str})\n",
    "    return detect_cat(stg_visual_data, IMG_SIZE)\n",
    "\n",
    "trans_visual_cat_detection = build_trans_visual_cat_detection()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "807cf876",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 4 – Build `mrt_experiences` from the transformed tables\n",
    "# ------------------------------------------------------------------\n",
    "from collections import deque\n",
    "\n",
    "def _centroid(det_entry):\n",
    "    \"\"\"Return centroid x,y for detection dict (expects xmin,xmax,ymin,ymax).\"\"\"\n",
    "    return (det_entry[\"xmin\"] + det_entry[\"xmax\"]) / 2, (det_entry[\"ymin\"] + det_entry[\"ymax\"]) / 2\n",
    "\n",
    "def build_mrt_experiences(aud_df: pd.DataFrame,\n",
    "                          imu_df: pd.DataFrame,\n",
    "                          vis_df: pd.DataFrame,\n",
    "                          n_frames: int = 12):\n",
    "    rows = []\n",
    "    for fid in sorted(vis_df[\"frame_id\"].unique()):\n",
    "        aud_window = aud_df[aud_df[\"frame_id\"] <= fid].tail(n_frames)\n",
    "        imu_window = imu_df[imu_df[\"frame_id\"] <= fid].tail(n_frames)\n",
    "        vis_window = vis_df[vis_df[\"frame_id\"] <= fid].tail(n_frames)\n",
    "\n",
    "        if len(vis_window) < n_frames:\n",
    "            rows.append({\n",
    "                \"experience_id\": fid,\n",
    "                \"timestamp\": np.nan,\n",
    "                \"is_cat_voice\": np.nan,\n",
    "                \"is_human_voice\": np.nan,\n",
    "                \"human_voice_sequence\": np.nan,\n",
    "                \"cat_voice_sequence\": np.nan,\n",
    "                \"meow_loudness\": np.nan,\n",
    "                \"cat_detected\": np.nan,\n",
    "                \"cat_position_x\": np.nan,\n",
    "                \"cat_position_y\": np.nan,\n",
    "                \"cat_movement_direction\": np.nan,\n",
    "                \"cat_activity_level\": np.nan,\n",
    "                \"cat_distance_change\": np.nan,\n",
    "                \"movement_intensity\": np.nan,\n",
    "                \"cat_interaction_detected\": np.nan,\n",
    "            })\n",
    "            continue\n",
    "\n",
    "        # Audio aggregates\n",
    "        aud_is_cat   = aud_window[\"is_cat_voice\"].fillna(False)\n",
    "        aud_is_human = aud_window[\"is_human_voice\"].fillna(False)\n",
    "        human_seq    = aud_window.loc[aud_is_human, \"frame_id\"].tolist()\n",
    "        cat_seq      = aud_window.loc[aud_is_cat,   \"frame_id\"].tolist()\n",
    "        meow_loudness= aud_window[\"meow_loudness\"].mode().iloc[0] if not aud_window[\"meow_loudness\"].empty else np.nan\n",
    "\n",
    "        # IMU aggregates\n",
    "        movement_intensity = imu_window[\"movement_intensity\"].mean()\n",
    "        cat_interaction    = imu_window[\"cat_interaction_detected\"].any()\n",
    "\n",
    "        # Vision aggregates – use last row\n",
    "        vis_last = vis_window.iloc[-1]\n",
    "        cat_detected = bool(vis_last[\"is_cat_detected\"])\n",
    "        cat_x = cat_y = np.nan\n",
    "        if cat_detected and vis_last[\"raw_detection\"]:\n",
    "            for det in vis_last[\"raw_detection\"]:\n",
    "                if det.get(\"name\") == \"cat\":\n",
    "                    cat_x, cat_y = _centroid(det)\n",
    "                    break\n",
    "\n",
    "        # Cat motion between last two frames\n",
    "        cat_movement_direction = np.nan\n",
    "        cat_activity_level = np.nan\n",
    "        cat_distance_change = np.nan\n",
    "        if len(vis_window) >= 2:\n",
    "            prev = vis_window.iloc[-2]\n",
    "            if prev[\"raw_detection\"] and vis_last[\"raw_detection\"]:\n",
    "                prev_det = next((d for d in prev[\"raw_detection\"] if d.get(\"name\") == \"cat\"), None)\n",
    "                curr_det = next((d for d in vis_last[\"raw_detection\"] if d.get(\"name\") == \"cat\"), None)\n",
    "                if prev_det and curr_det:\n",
    "                    prev_x, prev_y = _centroid(prev_det)\n",
    "                    dx = cat_x - prev_x\n",
    "                    dy = cat_y - prev_y\n",
    "                    if abs(dx) > abs(dy):\n",
    "                        cat_movement_direction = \"right\" if dx > 0 else \"left\"\n",
    "                    else:\n",
    "                        cat_movement_direction = \"down\" if dy > 0 else \"up\"\n",
    "                    cat_activity_level = \"moving\" if max(abs(dx), abs(dy)) > 3 else \"still\"\n",
    "                    if dy < -2:\n",
    "                        cat_distance_change = \"closer\"\n",
    "                    elif dy > 2:\n",
    "                        cat_distance_change = \"farther\"\n",
    "                    else:\n",
    "                        cat_distance_change = \"no_change\"\n",
    "\n",
    "        rows.append({\n",
    "            \"experience_id\"           : fid,\n",
    "            \"timestamp\"               : vis_last.get(\"timestamp\", np.nan),\n",
    "            \"is_cat_voice\"            : bool(aud_is_cat.any()),\n",
    "            \"is_human_voice\"          : bool(aud_is_human.any()),\n",
    "            \"human_voice_sequence\"    : human_seq,\n",
    "            \"cat_voice_sequence\"      : cat_seq,\n",
    "            \"meow_loudness\"           : meow_loudness,\n",
    "            \"cat_detected\"            : cat_detected,\n",
    "            \"cat_position_x\"          : cat_x,\n",
    "            \"cat_position_y\"          : cat_y,\n",
    "            \"cat_movement_direction\"  : cat_movement_direction,\n",
    "            \"cat_activity_level\"      : cat_activity_level,\n",
    "            \"cat_distance_change\"     : cat_distance_change,\n",
    "            \"movement_intensity\"      : movement_intensity,\n",
    "            \"cat_interaction_detected\": cat_interaction\n",
    "        })\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "mrt_experiences = build_mrt_experiences(trans_audio_features,\n",
    "                                        trans_imu_features,\n",
    "                                        trans_visual_cat_detection)"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
